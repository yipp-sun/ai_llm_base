# PDF æ–‡æ¡£é—®ç­”ChatBot
## æœ¬åœ°ä¸Šä¼ æ–‡æ¡£
+ æ”¯æŒ **pdf**
+ æ”¯æŒ **txt**
+ æ”¯æŒ **doc/docx**

## é—®ç­”é¡µé¢
![](https://cdn.nlark.com/yuque/0/2024/png/2424104/1724400774841-0da24c4e-9a73-4bbf-bccb-bc7f05b67832.png)

## pythonç¯å¢ƒ
+ æ–°å»ºä¸€ä¸ª`requirements.txt`æ–‡ä»¶

```plain
streamlit
python-docx
PyPDF2
faiss-cpu
langchain
langchain-core
langchain-community
langchain-openai
```

+ ç„¶åå®‰è£…ç›¸åº”çš„åŒ…

```plain
pip install -r requirements.txt -U
```

## ä»£ç 
> åˆ›å»ºä¸€ä¸ª <font style="color:#080808;background-color:#ffffff;">pdf_search.py</font> æ–‡ä»¶ï¼Œ æŠŠä¸‹è¾¹çš„å¤åˆ¶è¿›å»  
æ³¨æ„ï¼šé…ç½®å¥½OPEN_API æ¥å£åœ°å€å’Œå¯†é’¥çš„ç¯å¢ƒå˜é‡
>

```python
#ç¤ºä¾‹ï¼špdf_search.py
# å¯¼å…¥Streamlitåº“ï¼Œç”¨äºåˆ›å»ºWebåº”ç”¨
import streamlit as st
# å¯¼å…¥é€’å½’å­—ç¬¦æ–‡æœ¬åˆ†å‰²å™¨ï¼Œç”¨äºå°†æ–‡æ¡£åˆ†å‰²æˆå°å—
from langchain.text_splitter import RecursiveCharacterTextSplitter
# å¯¼å…¥FAISSå‘é‡å­˜å‚¨ï¼Œç”¨äºå­˜å‚¨å’Œæ£€ç´¢æ–‡æ¡£åµŒå…¥
from langchain_community.vectorstores import FAISS
# å¯¼å…¥OpenAIèŠå¤©æ¨¡å‹
from langchain_openai import ChatOpenAI
# å¯¼å…¥OpenAIåµŒå…¥æ¨¡å‹ï¼Œç”¨äºç”Ÿæˆæ–‡æœ¬åµŒå…¥
from langchain_openai import OpenAIEmbeddings
# å¯¼å…¥Documentç±»ï¼Œç”¨äºå°è£…æ–‡æ¡£å†…å®¹å’Œå…ƒæ•°æ®
from langchain_core.documents import Document
# å¯¼å…¥å¯¹è¯æ£€ç´¢é“¾ï¼Œç”¨äºå¤„ç†å¯¹è¯å’Œæ£€ç´¢
from langchain.chains import ConversationalRetrievalChain
# å¯¼å…¥docxåº“ï¼Œç”¨äºå¤„ç†Wordæ–‡æ¡£
import docx
# å¯¼å…¥PyPDF2åº“ï¼Œç”¨äºå¤„ç†PDFæ–‡æ¡£
from PyPDF2 import PdfReader

# è®¾ç½®é¡µé¢é…ç½®ï¼ŒåŒ…æ‹¬æ ‡é¢˜ã€å›¾æ ‡å’Œå¸ƒå±€
st.set_page_config(page_title="æ–‡æ¡£é—®ç­”", page_icon=":robot:", layout="wide")

# è®¾ç½®é¡µé¢çš„CSSæ ·å¼
st.markdown(
    """<style>
.chat-message {
    padding: 1.5rem; border-radius: 0.5rem; margin-bottom: 1rem; display: flex
}
.chat-message.user {
    background-color: #2b313e
}
.chat-message.bot {
    background-color: #475063
}
.chat-message .avatar {
  width: 20%;
}
.chat-message .avatar img {
  max-width: 78px;
  max-height: 78px;
  border-radius: 50%;
  object-fit: cover;
}
.chat-message .message {
  width: 80%;
  padding: 0 1.5rem;
  color: #fff;
}
.stDeployButton {
            visibility: hidden;
        }
#MainMenu {visibility: hidden;}
footer {visibility: hidden;}

.block-container {
    padding: 2rem 4rem 2rem 4rem;
}

.st-emotion-cache-16txtl3 {
    padding: 3rem 1.5rem;
}
</style>
# """,
    unsafe_allow_html=True,
)

# å®šä¹‰æœºå™¨äººæ¶ˆæ¯æ¨¡æ¿
bot_template = """
<div class="chat-message bot">
    <div class="avatar">
        <img src="https://cdn.icon-icons.com/icons2/1371/PNG/512/robot02_90810.png" style="max-height: 78px; max-width: 78px; border-radius: 50%; object-fit: cover;">
    </div>
    <div class="message">{{MSG}}</div>
</div>
"""

# å®šä¹‰ç”¨æˆ·æ¶ˆæ¯æ¨¡æ¿
user_template = """
<div class="chat-message user">
    <div class="avatar">
        <img src="https://www.shareicon.net/data/512x512/2015/09/18/103160_man_512x512.png" >
    </div>    
    <div class="message">{{MSG}}</div>
</div>
"""

# ä»PDFæ–‡ä»¶ä¸­æå–æ–‡æœ¬
def get_pdf_text(pdf_docs):
    # å­˜å‚¨æå–çš„æ–‡æ¡£
    docs = []
    for document in pdf_docs:
        if document.type == "application/pdf":
            # è¯»å–PDFæ–‡ä»¶
            pdf_reader = PdfReader(document)
            for idx, page in enumerate(pdf_reader.pages):
                docs.append(
                    Document(
                        # æå–é¡µé¢æ–‡æœ¬
                        page_content=page.extract_text(),
                        # æ·»åŠ å…ƒæ•°æ®
                        metadata={"source": f"{document.name} on page {idx}"},
                    )
                )
        elif document.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
            # è¯»å–Wordæ–‡æ¡£
            doc = docx.Document(document)
            for idx, paragraph in enumerate(doc.paragraphs):
                docs.append(
                    Document(
                        # æå–æ®µè½æ–‡æœ¬
                        page_content=paragraph.text,
                        # æ·»åŠ å…ƒæ•°æ®
                        metadata={"source": f"{document.name} in paragraph {idx}"},
                    )
                )
        elif document.type == "text/plain":
            # è¯»å–çº¯æ–‡æœ¬æ–‡ä»¶
            text = document.getvalue().decode("utf-8")
            docs.append(Document(page_content=text, metadata={"source": document.name}))

    return docs

# å°†æ–‡æ¡£åˆ†å‰²æˆå°å—æ–‡æœ¬
def get_text_chunks(docs):
    # åˆ›å»ºæ–‡æœ¬åˆ†å‰²å™¨
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=0)
    # åˆ†å‰²æ–‡æ¡£
    docs_chunks = text_splitter.split_documents(docs)
    return docs_chunks

# åˆ›å»ºå‘é‡å­˜å‚¨
def get_vectorstore(docs_chunks):
    # åˆ›å»ºOpenAIåµŒå…¥æ¨¡å‹
    embeddings = OpenAIEmbeddings()
    # åˆ›å»ºFAISSå‘é‡å­˜å‚¨
    vectorstore = FAISS.from_documents(docs_chunks, embedding=embeddings)
    return vectorstore

# åˆ›å»ºå¯¹è¯æ£€ç´¢é“¾
def get_conversation_chain(vectorstore):
    # åˆ›å»ºOpenAIèŠå¤©æ¨¡å‹
    llm = ChatOpenAI(model="gpt-4o")
    conversation_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        # ä½¿ç”¨å‘é‡å­˜å‚¨ä½œä¸ºæ£€ç´¢å™¨
        retriever=vectorstore.as_retriever(),
        # è¿”å›æºæ–‡æ¡£
        return_source_documents=True,
    )
    return conversation_chain

# å¤„ç†ç”¨æˆ·è¾“å…¥å¹¶ç”Ÿæˆå“åº”
def handle_userinput_pdf(user_question):
    # è·å–èŠå¤©å†å²
    chat_history = st.session_state.chat_history
    # ç”Ÿæˆå“åº”
    response = st.session_state.conversation(
        {"question": user_question, "chat_history": chat_history}
    )
    # æ·»åŠ ç”¨æˆ·é—®é¢˜åˆ°èŠå¤©å†å²
    st.session_state.chat_history.append(("user", user_question))
    # æ·»åŠ æœºå™¨äººå›ç­”åˆ°èŠå¤©å†å²
    st.session_state.chat_history.append(("assistant", response["answer"]))

    # æ˜¾ç¤ºç”¨æˆ·é—®é¢˜
    st.write(
        user_template.replace("{{MSG}}", user_question),
        unsafe_allow_html=True,
    )

    # è·å–æºæ–‡æ¡£
    sources = response["source_documents"]
    # æå–æºæ–‡æ¡£åç§°
    source_names = set([i.metadata["source"] for i in sources])
    # åˆå¹¶æºæ–‡æ¡£åç§°
    src = "\n\n".join(source_names)
    src = f"\n\n> source : {src}"
    message = st.session_state.chat_history[-1]
    # æ˜¾ç¤ºæœºå™¨äººå›ç­”å’Œæºæ–‡æ¡£
    st.write(bot_template.replace("{{MSG}}", message[1] + src), unsafe_allow_html=True)

# æ˜¾ç¤ºèŠå¤©å†å²è®°å½•
def show_history():
    # è·å–èŠå¤©å†å²
    chat_history = st.session_state.chat_history
    for i, message in enumerate(chat_history):
        if i % 2 == 0:
            # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
            st.write(
                user_template.replace("{{MSG}}", message[1]),
                unsafe_allow_html=True,
            )
        else:
            # æ˜¾ç¤ºæœºå™¨äººæ¶ˆæ¯
            st.write(
                bot_template.replace("{{MSG}}", message[1]), unsafe_allow_html=True
            )

# ä¸»å‡½æ•°
def main():
    # æ˜¾ç¤ºé¡µé¢æ ‡é¢˜
    st.header("Chat with Documents")
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if "conversation" not in st.session_state:
        st.session_state.conversation = None
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    with st.sidebar:
        # æ˜¾ç¤ºä¾§è¾¹æ æ ‡é¢˜
        st.title("æ–‡æ¡£ç®¡ç†")
        # æ–‡ä»¶ä¸Šä¼ æ§ä»¶
        pdf_docs = st.file_uploader(
            "é€‰æ‹©æ–‡ä»¶",
            # æ”¯æŒçš„æ–‡ä»¶ç±»å‹
            type=["pdf", "txt", "doc", "docx"],
            # æ”¯æŒå¤šæ–‡ä»¶ä¸Šä¼ 
            accept_multiple_files=True,
        )
        if st.button(
                "å¤„ç†æ–‡æ¡£",
                # è®¾ç½®æœ€åæ“ä½œä¸ºpdf
                on_click=lambda: setattr(st.session_state, "last_action", "pdf"),
                use_container_width=True,
        ):
            if pdf_docs:
                # æ˜¾ç¤ºå¤„ç†ä¸­çš„æ—‹è½¬å™¨
                with st.spinner("Processing"):
                    # æå–PDFã€docã€txtæ–‡æœ¬
                    # chatgpt.pdf æ‹†åˆ†ä¸º3ä¸ªdoc
                    # knowledge.txt æ‹†åˆ†ä¸º1ä¸ªdoc
                    # news.docx æ‹†åˆ†ä¸º37ä¸ªdoc
                    docs = get_pdf_text(pdf_docs)
                    # åˆ†å‰²æ–‡æœ¬
                    docs_chunks = get_text_chunks(docs)
                    # åˆ›å»ºå‘é‡å­˜å‚¨
                    vectorstore = get_vectorstore(docs_chunks)
                    # åˆ›å»ºå¯¹è¯é“¾
                    st.session_state.conversation = get_conversation_chain(vectorstore)
            else:
                # æç¤ºç”¨æˆ·ä¸Šä¼ æ–‡ä»¶
                st.warning("è®°å¾—ä¸Šä¼ æ–‡ä»¶å“¦~~")

        def clear_history():
            # æ¸…ç©ºèŠå¤©å†å²
            st.session_state.chat_history = []

        if st.session_state.chat_history:
            # æ¸…ç©ºå¯¹è¯æŒ‰é’®
            st.button("æ¸…ç©ºå¯¹è¯", on_click=clear_history, use_container_width=True)

    with st.container():
        # è·å–ç”¨æˆ·è¾“å…¥
        user_question = st.chat_input("è¾“å…¥ç‚¹ä»€ä¹ˆ~")

    with st.container(height=400):
        # æ˜¾ç¤ºèŠå¤©å†å²
        show_history()
        if user_question:
            if st.session_state.conversation is not None:
                # å¤„ç†ç”¨æˆ·è¾“å…¥
                handle_userinput_pdf(user_question)
            else:
                # æç¤ºç”¨æˆ·ä¸Šä¼ æ–‡ä»¶
                st.warning("è®°å¾—ä¸Šä¼ æ–‡ä»¶å“¦~~")

# è¿è¡Œä¸»å‡½æ•°
if __name__ == "__main__":
    main()
```

## å¯åŠ¨
```powershell
streamlit run pdf_search.py
```



## é—®ç­”æ•ˆæœ
**é—®é¢˜1ï¼šchatgptä¸ºä»€ä¹ˆé‚£ä¹ˆç«çˆ†?**![](https://cdn.nlark.com/yuque/0/2024/png/2424104/1724400878852-52f054bd-2e41-4694-bc43-dbcf0ebd54d9.png)

**é—®é¢˜2ï¼š2024ä¸–ç•Œäººå·¥æ™ºèƒ½å¤§ä¼šå“ªå¤©å¼€å§‹?**

![](https://cdn.nlark.com/yuque/0/2024/png/2424104/1724401022867-ea991b96-598a-4b68-bdf8-51cd3a5d4ee5.png)

**é—®é¢˜3ï¼šPixarå…¬å¸æ˜¯åšä»€ä¹ˆçš„?**

![](https://cdn.nlark.com/yuque/0/2024/png/2424104/1724401096332-cce91ede-b48a-46d6-a9d9-baa2e7ea8e8b.png)

**é—®é¢˜4ï¼šé»„æ²³ä¸ºä»€ä¹ˆè¢«ç§°ä¸ºæ¯äº²æ²³?**

![](https://cdn.nlark.com/yuque/0/2024/png/2424104/1724401179826-3d6ce67f-9be4-444f-af9e-5efe9fe1b67a.png)

# åŸºäº Web URL çš„é—®ç­”ChatBot
## <font style="color:rgb(51, 51, 51);">å¯¼å…¥åº“</font>
<font style="color:rgb(51, 51, 51);">æˆ‘ä»¬å°†é¦–å…ˆå¯¼å…¥èŠå¤©æœºå™¨äººæ‰€éœ€çš„åº“ã€‚</font>

```python
#ç¤ºä¾‹ï¼šweb_search.py
import time
from datetime import datetime

import requests
import streamlit as st
import wikipedia
from bs4 import BeautifulSoup
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain_openai import OpenAIEmbeddings
from streamlit_chat import message
global docsearch
from langchain.globals import set_verbose
docsearch = None
```

## <font style="color:rgb(51, 51, 51);">çˆ¬å–ç»´åŸºç™¾ç§‘</font>
<font style="color:rgb(51, 51, 51);">æ„å»ºèŠå¤©æœºå™¨äººçš„ç¬¬ä¸€æ­¥æ˜¯è®¿é—®ç»´åŸºç™¾ç§‘æ–‡ç« å¹¶æå–å†…å®¹ã€‚è¯¥</font>`<font style="color:rgb(51, 51, 51);background-color:rgb(243, 244, 244);">get_wiki</font>`<font style="color:rgb(51, 51, 51);">å‡½æ•°æ¥å—æœç´¢è¯å¹¶è¿”å›æ•´é¡µå†…å®¹å’Œç»´åŸºç™¾ç§‘æ–‡ç« çš„æ‘˜è¦ã€‚è¯¥</font>`<font style="color:rgb(51, 51, 51);background-color:rgb(243, 244, 244);">wikipedia.summary</font>`<font style="color:rgb(51, 51, 51);">æ–¹æ³•æœç´¢æ‘˜è¦ï¼Œä»¥åŠ</font>`<font style="color:rgb(51, 51, 51);background-color:rgb(243, 244, 244);">requests</font>`<font style="color:rgb(51, 51, 51);">ç”¨äºè®¿é—®æ–‡ç« çš„ URL çš„æ¨¡å—ã€‚è¯¥</font>`<font style="color:rgb(51, 51, 51);background-color:rgb(243, 244, 244);">BeautifulSoup</font>`<font style="color:rgb(51, 51, 51);">æ¨¡å—ä½¿ç”¨åœ¨è§£æé¡µé¢çš„HTMLå†…å®¹ï¼Œè¯¥</font>`<font style="color:rgb(51, 51, 51);background-color:rgb(243, 244, 244);">content_div.find_all('p')</font>`<font style="color:rgb(51, 51, 51);">è¡Œä»é¡µé¢ä¸Šçš„æ®µè½ä¸­æå–æ–‡æœ¬ã€‚</font>

```python
def get_wiki(search):
    # å°†è¯­è¨€è®¾ç½®ä¸ºç®€ä½“ä¸­æ–‡ï¼ˆé»˜è®¤ä¸ºè‡ªåŠ¨æ£€æµ‹ï¼‰
    lang = "zh"

    """
    ä»ç»´åŸºç™¾ç§‘è·å–æ‘˜è¦
    """
    # set language to zh_CN (default is auto-detect)
    wikipedia.set_lang(lang)
    summary = wikipedia.summary(search, sentences=5)

    """
    æŠ“å–æ‰€è¯·æ±‚æŸ¥è¯¢çš„ç»´åŸºç™¾ç§‘é¡µé¢
    """

    # æ ¹æ®ç”¨æˆ·è¾“å…¥å’Œè¯­è¨€åˆ›å»ºURL
    url = f"https://{lang}.wikipedia.org/wiki/{search}"

    # å‘URLå‘é€GETè¯·æ±‚å¹¶è§£æHTMLå†…å®¹
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")

    # æå–é¡µé¢çš„ä¸»è¦å†…å®¹
    content_div = soup.find(id="mw-content-text")

    # æ‘˜å½•æ‰€æœ‰å†…å®¹æ®µè½
    paras = content_div.find_all('p')

    # å°†æ®µè½è¿æ¥æˆæ•´é¡µå†…å®¹
    full_page_content = ""
    for para in paras:
        full_page_content += para.text

    # æ‰“å°æ•´é¡µå†…å®¹
    return full_page_content, summary
```

## <font style="color:rgb(51, 51, 51);">è®¾ç½®ç”¨æˆ·ç•Œé¢</font> 
<font style="color:rgb(51, 51, 51);">æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨ Streamlit è®¾ç½®ç”¨æˆ·ç•Œé¢ã€‚æˆ‘ä»¬é¦–å…ˆåˆ›å»ºä¸€ä¸ªæ ‡é¢˜ï¼š</font>

```python
st.markdown("<h1 style='text-align: center; color: Black;'>åŸºäº Web URL çš„é—®ç­”</h1>", unsafe_allow_html=True)
```

<font style="color:rgb(51, 51, 51);">è¿™å°†ä¸ºèŠå¤©æœºå™¨äººåˆ›å»ºä¸€ä¸ªå¤§è€Œå±…ä¸­çš„æ ‡é¢˜ã€‚</font>

<font style="color:rgb(51, 51, 51);">ç¯å¢ƒå˜é‡é…ç½®å¥½</font><font style="color:#080808;background-color:#ffffff;">OPENAI_BASE_URLå’ŒOPENAI_API_KEY</font>

```python
setx OPENAI_BASE_URL "https://api.openai.com/v1"
setx OPENAI_API_KEY "sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
```

<font style="color:rgb(51, 51, 51);">ä¸€æ—¦ç”¨æˆ·è¾“å…¥ä»–ä»¬çš„ OpenAI å¯†é’¥ï¼Œæˆ‘ä»¬å°±ä¼šåˆå§‹åŒ– GPT æ¨¡å‹å¹¶è¦æ±‚ä»–ä»¬è¾“å…¥æœç´¢æŸ¥è¯¢ï¼Œè¯¥æŸ¥è¯¢å°†ç”¨äºæŠ“å–ç›¸å…³çš„ Wikipedia é¡µé¢ã€‚get_wiki()å‡½æ•°å°†è¿”å›æœç´¢æŸ¥è¯¢å’ŒæŠ“å–é¡µé¢çš„æ‘˜è¦ã€‚ç°åœ¨ï¼Œå¦‚æœå®ƒè¿”å›äº†ä¸€äº›å€¼ï¼Œåˆ™ Q&A å­—æ®µå°†è¢«æ¿€æ´»ï¼Œç”¨æˆ·å¯ä»¥æé—®ã€‚</font>

```python
search = st.text_input("è¯·è¾“å…¥è¦æ£€ç´¢çš„å…³é”®è¯")
if len(search):
    wiki_content, summary = get_wiki(search)

    if len(wiki_content):
        try:
            # åˆ›å»ºç”¨æˆ·å‘é€æ¶ˆæ¯çš„è¾“å…¥æ–‡æœ¬æ¡†
            st.write(summary)
            user_query = st.text_input("You: ", "", key="input")
            send_button = st.button("Send")
```

<font style="color:rgb(51, 51, 51);">ç°åœ¨ï¼Œæˆ‘ä»¬åˆå§‹åŒ–FAISSå‘é‡æ•°æ®åº“</font>

```python
def init_db(wiki_content):
    print("åˆå§‹åŒ–FAISSå‘é‡æ•°æ®åº“...")
    text_splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
    )
    texts = text_splitter.split_text(wiki_content)
    embeddings = OpenAIEmbeddings()
    global doc_search
    doc_search = FAISS.from_texts(texts, embeddings)
```

<font style="color:rgb(51, 51, 51);">å»ºç«‹ç´¢å¼•åï¼Œæˆ‘ä»¬å°±å¯ä»¥æŸ¥è¯¢ç”¨æˆ·çš„è¯·æ±‚ã€‚</font>

```python
# åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥è·å–æœºå™¨äººå“åº”
def get_bot_response(user_query):
    # åœ¨å‘é‡æ•°æ®åº“ä¸­è¿›è¡Œç›¸ä¼¼æ€§æœç´¢ï¼Œè¿”å›6ä¸ªç»“æœ
    docs = doc_search.similarity_search(user_query, K=6)
    main_content = user_query + "\n\n"
    # æ‹¼æ¥ç”¨æˆ·æŸ¥è¯¢å’Œç›¸ä¼¼çš„æ–‡æœ¬å†…å®¹
    for doc in docs:
        main_content += doc.page_content + "\n\n"
    messages.append(HumanMessage(content=main_content))
    # è°ƒç”¨OpenAIæ¥å£è·å–å“åº”
    ai_response = chat.invoke(messages).content
    # å°†åˆšåˆšæ·»åŠ çš„ HumanMessage ä» messages åˆ—è¡¨ä¸­ç§»é™¤ã€‚è¿™æ ·åšçš„åŸå› æ˜¯ï¼Œmain_content åŒ…å«äº†ç”¨æˆ·çš„åŸå§‹æŸ¥è¯¢å’Œç›¸ä¼¼æ–‡æœ¬å†…å®¹ï¼Œ
    # ä½†åœ¨å®é™…çš„å¯¹è¯å†å²ä¸­ï¼Œæˆ‘ä»¬åªå¸Œæœ›ä¿ç•™ç”¨æˆ·çš„åŸå§‹æŸ¥è¯¢å’Œ AI çš„å“åº”ï¼Œè€Œä¸æ˜¯åŒ…å«ç›¸ä¼¼æ–‡æœ¬å†…å®¹çš„æŸ¥è¯¢ã€‚
    messages.pop()
    # å°†ç”¨æˆ·æŸ¥è¯¢æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨
    messages.append(HumanMessage(content=user_query))
    # å°†ç”¨æˆ·æŸ¥è¯¢æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨
    messages.append(AIMessage(content=ai_response))
    return ai_response
```

<font style="color:rgb(51, 51, 51);">å°±è¿™æ ·ï¼ä½ å°±æ‹¥æœ‰äº†ä¸“å±äºæ‚¨çš„å‹å¥½æœºå™¨äººï¼Œå®ƒå¯ä»¥å›ç­”æ‚¨å…³äºç»´åŸºç™¾ç§‘æ–‡ç« çš„æŸ¥è¯¢ã€‚</font>



## é—®ç­”æ•ˆæœ
**é—®é¢˜1ï¼šé»„æ²³**

![](https://cdn.nlark.com/yuque/0/2024/png/2424104/1724398479647-ac19e53e-3bf8-48c7-aa72-1cd85392afcf.png)

**é—®é¢˜2ï¼šé»„æ²³ä¸ºä»€ä¹ˆæ˜¯ä¸–ç•Œä¸Šå«æ²™é‡æœ€é«˜çš„æ²³æµ?**

![](https://cdn.nlark.com/yuque/0/2024/png/2424104/1724398492510-e3cecc20-9733-4a55-8171-c2794f4b53cc.png)

# åŸºäº SQL çš„ CSV æ•°æ®åˆ†æé—®ç­” Web Search é›†æˆ
## éœ€æ±‚
åŸºäº LangChain å’Œ Streamlit çš„ Web åº”ç”¨ï¼Œç”¨äºä½¿ç”¨ LLM å’ŒåµŒå…¥ä» SQLite æ•°æ®åº“ä¸­æœç´¢ç›¸å…³çš„offerã€‚ç”¨æˆ·å¯ä»¥è¾“å…¥ä¸å“ç‰Œã€ç±»åˆ«æˆ–é›¶å”®å•†ç›¸å…³çš„æœç´¢æŸ¥è¯¢ï¼Œä¹Ÿæ”¯æŒé€šè¿‡SQLè¯­å¥è¿›è¡Œæœç´¢ï¼Œåº”ç”¨ç¨‹åºå°†ä»æ•°æ®åº“ä¸­æ£€ç´¢å¹¶æ˜¾ç¤ºç›¸å…³çš„offerã€‚è¯¥åº”ç”¨ä½¿ç”¨ OpenAI API è¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†å’ŒåµŒå…¥ç”Ÿæˆã€‚

SQLite å®˜ç½‘ï¼š[https://www.sqlite.org/pragma.html#toc](https://www.sqlite.org/pragma.html#toc)

SQLite ä½¿ç”¨æ‰‹å†Œï¼š[https://www.runoob.com/sqlite/sqlite-select.html](https://www.runoob.com/sqlite/sqlite-select.html)



## æ–¹æ³•
+ **ç›®æ ‡**ï¼šè¯¥æ–¹æ³•çš„ç›®æ ‡æ˜¯åŸºäºäº§å“ç±»åˆ«ã€å“ç‰Œæˆ–é›¶å”®å•†æŸ¥è¯¢ä» `offer_retailer` è¡¨ä¸­æå–ç›¸å…³çš„offerã€‚é‰´äºæ‰€éœ€æ•°æ®åˆ†æ•£åœ¨ `data` ç›®å½•ä¸­çš„å¤šä¸ªè¡¨ä¸­ï¼Œé‡‡ç”¨äº†è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥ä¿ƒè¿›æ™ºèƒ½æ•°æ®åº“æŸ¥è¯¢ã€‚
+ **æ•°æ®åº“å‡†å¤‡**ï¼šæœ€åˆï¼Œä½¿ç”¨å­˜å‚¨åœ¨ `data` ç›®å½•ä¸­çš„ `.csv` æ–‡ä»¶æ„å»ºäº†ä¸€ä¸ªæœ¬åœ° SQLite æ•°æ®åº“ã€‚è¿™æ˜¯é€šè¿‡ `sqlite3` å’Œ `pandas` åº“å®ç°çš„ã€‚
+ **LLM é›†æˆ**ï¼šé€šè¿‡ `langchain_experimental.sql.SQLDatabaseChain` å®ç°äº†è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸æœ¬åœ° SQLite æ•°æ®åº“çš„æœ‰æ•ˆäº¤äº’ã€‚
+ **æç¤ºå·¥ç¨‹**ï¼šè¯¥æ–¹æ³•çš„ä¸€ä¸ªé‡è¦æ–¹é¢æ˜¯åˆ¶å®šåˆé€‚çš„æç¤ºï¼Œä»¥æŒ‡å¯¼ LLM æœ€ä½³åœ°æ£€ç´¢å’Œæ ¼å¼åŒ–æ•°æ®åº“æ¡ç›®ã€‚é€šè¿‡å¤šæ¬¡è¿­ä»£å’Œå®éªŒæ¥å¾®è°ƒè¿™ä¸ªæç¤ºã€‚
+ **ç›¸ä¼¼åº¦è¯„åˆ†**ï¼šä¸ºäº†ç¡®å®šæ£€ç´¢ç»“æœä¸æŸ¥è¯¢çš„ç›¸å…³æ€§ï¼Œè¿›è¡Œäº†ä½™å¼¦ç›¸ä¼¼åº¦æ¯”è¾ƒã€‚ä½¿ç”¨ `langchain_openai.OpenAIEmbeddings` ç”ŸæˆåµŒå…¥è¿›è¡Œæ¯”è¾ƒï¼Œä»è€Œå¯¹ç»“æœè¿›è¡Œæ’åºã€‚
+ **Streamlit é›†æˆ**ï¼šæœ€åä¸€æ­¥æ˜¯è§£æ LLM çš„è¾“å‡ºï¼Œå¹¶å›´ç»•å®ƒæ„å»ºä¸€ä¸ªç”¨æˆ·å‹å¥½çš„ Streamlit åº”ç”¨ï¼Œå…è®¸ç”¨æˆ·è¿›è¡Œäº¤äº’å¼æœç´¢ã€‚

## ç¯å¢ƒ
åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ»¡è¶³ä»¥ä¸‹è¦æ±‚ï¼š

+ Python 3.12.4 æˆ–æ›´é«˜ç‰ˆæœ¬
+ OpenAI API å¯†é’¥
+ åŒ…å«ä»¥ä¸‹è¡¨çš„ SQLite æ•°æ®åº“ï¼š`brand_category`ï¼Œ`categories` å’Œ `offer_retailer`

å®‰è£…æ‰€éœ€çš„åŒ…ï¼š

```bash
pip install -r requirements.txt
```

ç¡®ä¿æ‚¨çš„ SQLite æ•°æ®åº“å·²è®¾ç½®å¥½ï¼Œå¹¶åŒ…å«å¿…è¦çš„è¡¨ï¼ˆ`brand_category`ï¼Œ`categories`ï¼Œ`offer_retailer`ï¼‰ã€‚



<font style="color:#DF2A3F;">æ³¨æ„ï¼šstreamlitç‰ˆæœ¬éœ€è¦</font><font style="color:#DF2A3F;background-color:#ffffff;"><1.30ï¼Œä¸€èˆ¬ä¸º1.29.0ï¼Œå¦åˆ™å¯åŠ¨ä¼šæŠ¥ä»¥ä¸‹é”™è¯¯</font>

![](https://cdn.nlark.com/yuque/0/2024/png/2424104/1729647910330-b2757326-ba90-4ecf-adcd-c8c5e6cf721f.png)



## ä»£ç 
```python
#ç¤ºä¾‹ï¼šcsv_search.py
import os
# å¯¼å…¥æ­£åˆ™è¡¨è¾¾å¼æ¨¡å—
import re
import sqlite3
import pandas as pd
import streamlit as st
from llm import RetrievalLLM

# æ•°æ®æ–‡ä»¶è·¯å¾„
DATA_PATH = 'data'
# æ•°æ®è¡¨åç§°
TABLES = ('brand_category', 'categories', 'offer_retailer')
# æ•°æ®åº“åç§°
DB_NAME = 'offer_db.sqlite'
# æç¤ºæ¨¡æ¿
PROMPT_TEMPLATE = """
                ä½ ä¼šæ¥æ”¶åˆ°ä¸€ä¸ªæŸ¥è¯¢ï¼Œä½ çš„ä»»åŠ¡æ˜¯ä»`offer_retailer`è¡¨ä¸­çš„`OFFER`å­—æ®µæ£€ç´¢ç›¸å…³offerã€‚
                æŸ¥è¯¢å¯èƒ½æ˜¯æ··åˆå¤§å°å†™çš„ï¼Œæ‰€ä»¥ä¹Ÿè¦æœç´¢å¤§å†™ç‰ˆæœ¬çš„æŸ¥è¯¢ã€‚
                é‡è¦çš„æ˜¯ï¼Œä½ å¯èƒ½éœ€è¦ä½¿ç”¨æ•°æ®åº“ä¸­å…¶ä»–è¡¨çš„ä¿¡æ¯ï¼Œå³ï¼š`brand_category`, `categories`, `offer_retailer`ï¼Œæ¥æ£€ç´¢æ­£ç¡®çš„offerã€‚
                ä¸è¦è™šæ„offerã€‚å¦‚æœåœ¨`offer_retailer`è¡¨ä¸­æ‰¾ä¸åˆ°offerï¼Œè¿”å›å­—ç¬¦ä¸²ï¼š`NONE`ã€‚
                å¦‚æœä½ èƒ½ä»`offer_retailer`è¡¨ä¸­æ£€ç´¢åˆ°offerï¼Œç”¨åˆ†éš”ç¬¦`#`åˆ†éš”æ¯ä¸ªofferã€‚ä¾‹å¦‚ï¼Œè¾“å‡ºåº”è¯¥æ˜¯è¿™æ ·çš„ï¼š`offer1#offer2#offer3`ã€‚
                å¦‚æœSQLResultä¸ºç©ºï¼Œè¿”å›`None`ã€‚ä¸è¦ç”Ÿæˆä»»ä½•offerã€‚
                è¿™æ˜¯æŸ¥è¯¢ï¼š`{}`
                """

# Streamlitåº”ç”¨æ ‡é¢˜
st.title("æœç´¢offer ğŸ”")

# è¿æ¥SQLiteæ•°æ®åº“
conn = sqlite3.connect('offer_db.sqlite')

# åˆ¤æ–­æ˜¯å¦æ˜¯SQLæŸ¥è¯¢çš„å‡½æ•°
def is_sql_query(query):
    # å®šä¹‰ä¸€ä¸ªåŒ…å«å¸¸è§ SQL å…³é”®å­—çš„åˆ—è¡¨
    sql_keywords = [
        'SELECT', 'INSERT', 'UPDATE', 'DELETE', 'CREATE', 'DROP', 'ALTER',
        'TRUNCATE', 'MERGE', 'CALL', 'EXPLAIN', 'DESCRIBE', 'SHOW'
    ]

    # å»æ‰æŸ¥è¯¢å­—ç¬¦ä¸²ä¸¤ç«¯çš„ç©ºç™½å­—ç¬¦å¹¶è½¬æ¢ä¸ºå¤§å†™
    query_upper = query.strip().upper()

    # éå† SQL å…³é”®å­—åˆ—è¡¨
    for keyword in sql_keywords:
        # å¦‚æœæŸ¥è¯¢å­—ç¬¦ä¸²ä»¥æŸä¸ª SQL å…³é”®å­—å¼€å¤´ï¼Œè¿”å› True
        if query_upper.startswith(keyword):
            return True

    # å®šä¹‰ä¸€ä¸ªæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼Œç”¨äºåŒ¹é…ä»¥ SQL å…³é”®å­—å¼€å¤´çš„å­—ç¬¦ä¸²
    sql_pattern = re.compile(
        r'^\s*(SELECT|INSERT|UPDATE|DELETE|CREATE|DROP|ALTER|TRUNCATE|MERGE|CALL|EXPLAIN|DESCRIBE|SHOW)\s+',
        re.IGNORECASE  # å¿½ç•¥å¤§å°å†™
    )

    # å¦‚æœæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œè¿”å› True
    if sql_pattern.match(query):
        return True

    # å¦‚æœæŸ¥è¯¢å­—ç¬¦ä¸²ä¸ç¬¦åˆä»»ä½• SQL å…³é”®å­—æ¨¡å¼ï¼Œè¿”å› False
    return False


# åˆ›å»ºä¸€ä¸ªè¡¨å•ç”¨äºæœç´¢
with st.form("search_form"):
    # è¾“å…¥æ¡†ç”¨äºè¾“å…¥æŸ¥è¯¢
    query = st.text_input("é€šè¿‡ç±»åˆ«ã€å“ç‰Œæˆ–å‘å¸ƒå•†æœç´¢offerã€‚")
    # æäº¤æŒ‰é’®
    submitted = st.form_submit_button("æœç´¢")
    # å®ä¾‹åŒ–RetrievalLLMç±»
    retrieval_llm = RetrievalLLM(
        data_path=DATA_PATH,
        tables=TABLES,
        db_name=DB_NAME,
        openai_api_key=os.getenv('OPENAI_API_KEY'),
    )
    # å¦‚æœè¡¨å•æäº¤
    if submitted:
        # å¦‚æœè¾“å…¥å†…å®¹æ˜¯SQLè¯­å¥ï¼Œåˆ™æ˜¾ç¤ºSQLæ‰§è¡Œç»“æœ
        if is_sql_query(query):
            st.write(pd.read_sql_query(query, conn))
        # å¦åˆ™ä½¿ç”¨LLMä»æ•°æ®åº“ä¸­æ£€ç´¢offer
        else:
            # ä½¿ç”¨RetrievalLLMå®ä¾‹æ£€ç´¢offer
            retrieved_offers = retrieval_llm.retrieve_offers(
                PROMPT_TEMPLATE.format(query)
            )
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›¸å…³offer
            if not retrieved_offers:
                st.text("æœªæ‰¾åˆ°ç›¸å…³offerã€‚")
            else:
                # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„offer
                st.table(retrieval_llm.parse_output(retrieved_offers, query))

```



 

```python
#ç¤ºä¾‹ï¼šllm.py
import sqlite3
import numpy as np
import pandas as pd
from langchain_openai import OpenAIEmbeddings
from langchain_openai import OpenAI
from langchain_community.utilities import SQLDatabase
from langchain_experimental.sql import SQLDatabaseChain


class RetrievalLLM:
    """ä¸€ä¸ªç±»ï¼Œç”¨äºä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ£€ç´¢å’Œé‡æ–°æ’åºofferã€‚

    å‚æ•°:
        data_path (str): åŒ…å«æ•°æ®CSVæ–‡ä»¶çš„ç›®å½•è·¯å¾„ã€‚
        tables (list[str]): æ•°æ®CSVæ–‡ä»¶çš„åç§°åˆ—è¡¨ã€‚
        db_name (str): ç”¨äºå­˜å‚¨æ•°æ®çš„SQLiteæ•°æ®åº“åç§°ã€‚
        openai_api_key (str): OpenAI APIå¯†é’¥ã€‚

    å±æ€§:
        data_path (str): åŒ…å«æ•°æ®CSVæ–‡ä»¶çš„ç›®å½•è·¯å¾„ã€‚
        tables (list[str]): æ•°æ®CSVæ–‡ä»¶çš„åç§°åˆ—è¡¨ã€‚
        db_name (str): ç”¨äºå­˜å‚¨æ•°æ®çš„SQLiteæ•°æ®åº“åç§°ã€‚
        openai_api_key (str): OpenAI APIå¯†é’¥ã€‚
        db (SQLDatabase): SQLiteæ•°æ®åº“è¿æ¥ã€‚
        llm (OpenAI): OpenAI LLMå®¢æˆ·ç«¯ã€‚
        embeddings (OpenAIEmbeddings): OpenAIåµŒå…¥å®¢æˆ·ç«¯ã€‚
        db_chain (SQLDatabaseChain): ä¸LLMé›†æˆçš„SQLæ•°æ®åº“é“¾ã€‚
    """

    def __init__(self, data_path, tables, db_name, openai_api_key):
        # åˆå§‹åŒ–ç±»å±æ€§
        self.data_path = data_path
        self.tables = tables
        self.db_name = db_name
        self.openai_api_key = openai_api_key

        # è¯»å–CSVæ–‡ä»¶å¹¶å­˜å‚¨åˆ°æ•°æ®å¸§å­—å…¸ä¸­
        dfs = {}
        for table in self.tables:
            dfs[table] = pd.read_csv(f"{self.data_path}/{table}.csv")

        # å°†æ•°æ®å¸§å†™å…¥SQLiteæ•°æ®åº“
        with sqlite3.connect(self.db_name) as local_db:
            for table, df in dfs.items():
                df.to_sql(table, local_db, if_exists="replace")

        # åˆ›å»ºSQLæ•°æ®åº“è¿æ¥
        self.db = SQLDatabase.from_uri(f"sqlite:///{self.db_name}")
        # åˆ›å»ºOpenAI LLMå®¢æˆ·ç«¯
        self.llm = OpenAI(
            temperature=0, verbose=True, openai_api_key=self.openai_api_key
        )
        # åˆ›å»ºOpenAIåµŒå…¥å®¢æˆ·ç«¯
        self.embeddings = OpenAIEmbeddings(openai_api_key=self.openai_api_key)
        # åˆ›å»ºSQLæ•°æ®åº“é“¾
        self.db_chain = SQLDatabaseChain.from_llm(self.llm, self.db)
        self.allow_reuse = True

    def retrieve_offers(self, prompt):
        """ä½¿ç”¨LLMä»æ•°æ®åº“ä¸­æ£€ç´¢offerã€‚

        å‚æ•°:
            prompt (str): ç”¨äºæ£€ç´¢offerçš„æç¤ºã€‚

        è¿”å›:
            list[str]: æ£€ç´¢åˆ°çš„offeråˆ—è¡¨ã€‚
        """

        # è¿è¡ŒSQLæ•°æ®åº“é“¾ä»¥æ£€ç´¢offer
        retrieved_offers = self.db_chain.run(prompt)
        # å¦‚æœretrieved_offersæ˜¯"None"ï¼Œåˆ™è¿”å›Noneï¼Œå¦åˆ™è¿”å›æ£€ç´¢åˆ°çš„offer
        return None if retrieved_offers == "None" else retrieved_offers

    def get_embeddings(self, documents):
        """ä½¿ç”¨LLMè·å–æ–‡æ¡£çš„åµŒå…¥ã€‚

        å‚æ•°:
            documents (list[str]): æ–‡æ¡£åˆ—è¡¨ã€‚

        è¿”å›:
            np.ndarray: åŒ…å«æ–‡æ¡£åµŒå…¥çš„NumPyæ•°ç»„ã€‚
        """

        # å¦‚æœæ–‡æ¡£åˆ—è¡¨åªæœ‰ä¸€ä¸ªæ–‡æ¡£ï¼Œå°†å•ä¸ªæ–‡æ¡£çš„åµŒå…¥è½¬æ¢ä¸ºNumpyæ•°ç»„
        if len(documents) == 1:
            return np.asarray(self.embeddings.embed_query(documents[0]))
        else:
            # å¦åˆ™è·å–æ¯ä¸ªæ–‡æ¡£çš„åµŒå…¥å¹¶å­˜å‚¨åˆ°åˆ—è¡¨ä¸­
            embeddings_list = []
            for document in documents:
                embeddings_list.append(self.embeddings.embed_query(document))
            return np.asarray(embeddings_list)

    def parse_output(self, retrieved_offers, query):
        """è§£æretrieve_offers()æ–¹æ³•çš„è¾“å‡ºå¹¶è¿”å›ä¸€ä¸ªæ•°æ®å¸§ã€‚

        å‚æ•°:
            retrieved_offers (list[str]): æ£€ç´¢åˆ°çš„offeråˆ—è¡¨ã€‚
            query (str): ç”¨äºæ£€ç´¢offerçš„æŸ¥è¯¢ã€‚

        è¿”å›:
            pd.DataFrame: åŒ…å«åŒ¹é…ç›¸ä¼¼åº¦å’Œofferçš„æ•°æ®å¸§ã€‚
        """

        # åˆ†å‰²æ£€ç´¢åˆ°çš„offer
        top_offers = retrieved_offers.split("#")

        # è·å–æŸ¥è¯¢çš„åµŒå…¥
        query_embedding = self.get_embeddings([query])
        # è·å–offerçš„åµŒå…¥
        offer_embeddings = self.get_embeddings(top_offers)

        # offer_embeddingsæ˜¯ä¸€ä¸ªäºŒç»´çš„Numpyæ•°ç»„ï¼ŒåŒ…å«å¤šä¸ªofferçš„åµŒå…¥å‘é‡ã€‚
        # query_embeddingæ˜¯ä¸€ä¸ªäºŒç»´çš„Numpyæ•°ç»„ï¼ŒåŒ…å«æŸ¥è¯¢çš„åµŒå…¥å‘é‡ã€‚
        # query_embedding.Tæ˜¯æŸ¥è¯¢åµŒå…¥çš„è½¬ç½®ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªåˆ—å‘é‡ï¼Œä¾¿äºè¿›è¡ŒçŸ©é˜µä¹˜æ³•ã€‚
        # np.dot()è®¡ç®—æ¯ä¸ªofferåµŒå…¥å‘é‡ä¸æŸ¥è¯¢åµŒå…¥å‘é‡ä¹‹é—´çš„ç‚¹ç§¯ï¼ˆå†…ç§¯ï¼‰ï¼Œç»“æœæ˜¯ä¸€ä¸ªäºŒç»´æ•°ç»„ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ è¡¨ç¤ºä¸€ä¸ªofferä¸æŸ¥è¯¢ä¹‹é—´çš„ç›¸ä¼¼åº¦åˆ†æ•°ã€‚
        # .flatten() å°†äºŒç»´æ•°ç»„è½¬æ¢ä¸ºä¸€ç»´æ•°ç»„ï¼Œå¾—åˆ°æ¯ä¸ª offer ä¸æŸ¥è¯¢ä¹‹é—´çš„ç›¸ä¼¼åº¦åˆ†æ•°åˆ—è¡¨ã€‚
        sim_scores = np.dot(offer_embeddings, query_embedding.T).flatten()
        # è®¡ç®—ç›¸ä¼¼åº¦å¾—åˆ†ï¼Œè½¬æ¢ä¸ºç™¾åˆ†æ¯”å½¢å¼
        sim_scores = [p * 100 for p in sim_scores]

        # åˆ›å»ºæ•°æ®å¸§å¹¶æŒ‰ç›¸ä¼¼åº¦æ’åº
        df = (
            pd.DataFrame({"åŒ¹é…ç›¸ä¼¼åº¦ %": sim_scores, "offer": top_offers})
            .sort_values(by=["åŒ¹é…ç›¸ä¼¼åº¦ %"], ascending=False)
            .reset_index(drop=True)
        )
        df.index += 1
        return df

```

## è¿è¡Œ
æœ¬åœ°è¿è¡Œåº”ç”¨

```bash
streamlit  csv_search.py
```

åº”ç”¨è¿è¡Œåï¼Œæ‰“å¼€æµè§ˆå™¨å¹¶å¯¼èˆªåˆ° `http://localhost:8501` è®¿é—®offeræœç´¢ç•Œé¢ã€‚

1. åœ¨æ–‡æœ¬è¾“å…¥æ¡†ä¸­è¾“å…¥æ‚¨çš„æœç´¢æŸ¥è¯¢ï¼ˆå“ç‰Œã€ç±»åˆ«æˆ–é›¶å”®å•†ï¼‰ã€‚
2. ç‚¹å‡»â€œæœç´¢â€æŒ‰é’®å¯åŠ¨æœç´¢ã€‚
3. åŒ¹é…æŸ¥è¯¢çš„ç›¸å…³offerå°†ä»¥è¡¨æ ¼å½¢å¼æ˜¾ç¤ºã€‚



## é—®ç­”æ•ˆæœ
**é—®é¢˜1ï¼šselect * from categories**

![](https://cdn.nlark.com/yuque/0/2024/png/2424104/1724401368100-c44f11a3-5be1-434c-b721-e2ada0608fe0.png)

**é—®é¢˜2ï¼šselect CATEGORY_ID from categories**

![](https://cdn.nlark.com/yuque/0/2024/png/2424104/1724401403726-e204fe12-e3e0-4832-964b-e042e934a77d.png)

**é—®é¢˜3ï¼šRED GOLD**

![](https://cdn.nlark.com/yuque/0/2024/png/2424104/1724405445966-2d9be43d-89e6-44b0-bb4f-82d27529ec12.png)

